# nanobot-auto 研究概述

> 用于 Presentation | 2025-02-16

---

## 1. 动机：为什么做这个研究？

### 起点

2024-2025 年，LLM 驱动的 Coding Agent（如 Cursor、Codex、Devin）展示了强大的代码生成能力。但它们都有一个共同特征：**人类在环 (Human-in-the-Loop)**。人类发现问题 → 人类派任务 → Agent 执行 → 人类验证。

一个自然的问题浮现：

> **如果移除人类——让 Agent 自己发现问题、自己修复、自己验证、自己部署——会发生什么？**

### nanobot-auto 的诞生

nanobot-auto 就是对这个问题的实验性回答。它是一个运行在生产环境中的 AI Agent，每天自动执行三步循环：

```
1. 分析自己的运行日志，发现问题 (log-miner)
2. 调用 Codex 生成修复代码，创建 PR (auto-dev)
3. 检查 CI 结果，合并通过的 PR，重启自己 (auto-merge)
```

**它修改的不是别人的代码，而是自己的代码。** 每一次循环后，运行的系统就不再是修改前的那个系统了。

### 从"能跑"到"需要理论"

初始原型可以工作，但我们很快发现一个问题：**我们不知道它到底在变好还是变差。** CI 通过 ≠ 代码质量提高。没有 error ≠ 系统改进。

更深层的问题是：这类系统会遇到哪些**根本性的困难**？有没有已知的理论框架可以帮助我们理解和预测它的行为？

这就是本研究的出发点。

---

## 2. 相关工作

### SICA (Self-Improving Coding Agent, Robeyns et al., 2025)

最直接的相关工作。SICA 让 Agent 基于自身的 benchmark 执行历史来优化自己的工具集和编排代码。

**SICA 与 nanobot-auto 的关键区别：**

| | SICA | nanobot-auto |
|---|------|-------------|
| 修改范围 | Agent 的工具和入口代码 (meta-level) | 整个项目代码库 (object-level) |
| 反馈来源 | 固定 benchmark 分数 | 生产运行时日志 |
| 环境 | 沙箱（安全） | 生产（需要安全保障） |
| 观察者独立性 | ✅ benchmark 不受 Agent 修改影响 | ❌ 观察者就是被修改的系统 |

SICA 证明了 LLM Agent 自我改进的可行性。nanobot-auto 探索的是：**当这个过程放到生产环境、且 Agent 修改自身运行代码时，会出现什么新问题？**

### 其他理论基础

| 理论 | 在本研究中的作用 |
|------|----------------|
| **MAPE-K** (IBM, 2003) | 自主计算参考架构，用于分析 Agent 的功能分解 |
| **流程挖掘** (van der Aalst) | 用 pm4py 做 Agent 行为的确定性分析 |
| **OODA Loop** (Boyd) | 循环速度与触发策略的理论依据 |
| **DORA Metrics** (Google) | 软件交付效能度量，用于评估自演化效果 |

---

## 3. OCLSE：我们提出的框架

我们将 nanobot-auto 所代表的范式命名为 **OCLSE (Online Closed-Loop Self-Evolution)**。

核心研究问题：
> **由 LLM 驱动的软件系统，能否通过在线闭环实现持续的自我改进？它会遇到哪些根本性问题？**

### 三个核心问题

通过 TEP 对抗性推演，我们识别出三个 OCLSE 特有的问题（离线自改进系统不会遇到）：

**P1: 观测-修改耦合 (Observer-Modifier Coupling)**
> 观察问题的组件（log-miner）和被修改的系统是同一个。修改 log-miner 的代码后，下一轮的观察行为就变了——不是因为环境变了，而是因为观察者变了。

**P2: 自诱导分布漂移 (Self-Induced Distribution Shift)**
> 每次自我修改都改变了未来问题的分布。修好了 A 类 bug，A 类 bug 变少了，但可能冒出 B 类 bug。对 V(n) 版本有效的修复策略，对 V(n+1) 不一定有效。

**P3: 安全自修改 (Safe Self-Modification)**
> 与沙箱不同，生产环境中的自修改可能导致不可恢复的故障——比如改坏了 agent loop 自身，系统就无法自愈了。

### 度量体系：Code Health Vector H(t)

要回答"系统在变好还是变差"，需要一个多维度的时序指标：

| 类别 | 包含指标 | 来源 |
|------|---------|------|
| A: 代码质量 | 认知复杂度、技术债、代码重复率 | 静态分析 |
| B: 交付效能 | 修复前置时间、变更失败率 | GitHub API |
| C: Agent 效率 | 每次修复的 token 消耗、修复成功率 | API 日志 |
| D: 运行时健康 | 错误率、可用率 | 心跳监控 |

---

## 4. 架构设计

### 最小完备分解：Sense → Effect → Gate

通过 TEP 推演确认：闭环自修改系统**最少需要三个不可约角色**：

- **Sense (感知)** = log-miner：发现问题
- **Effect (执行)** = auto-dev：生成修复
- **Gate (门控)** = auto-merge：决定是否接受

为什么不能少？Effector 不能同时是 Gatekeeper（权限分离原则）。
为什么不用多？LLM 在一次推理中可以同时完成 Monitor + Analyze + Plan。

### 缺失的 K 层

对标 MAPE-K 发现：系统完全**没有跨周期记忆**。每个 cycle 从零开始分析，不知道上一轮做了什么。

解决方案：Fix Outcome Log（修复结果记录）+ Issue Type Registry（问题类型注册表）。

---

## 5. Recording & Analysis 策略

**核心洞见：自演化的质量上限 = 输入数据的质量上限。**

### Recording：Tool-Call 事件日志

Agent 的所有行为都是工具调用（读文件、写文件、执行命令、调 API）。在统一入口 `Tool.execute()` 处埋点，自动捕获所有 Agent 活动，输出 XES 格式事件日志。

实现成本：~20 行代码，改 1 个文件。

### Analysis：Process Mining + LLM 混合

| 确定性分析 (pm4py) | 语义分析 (LLM) |
|--------------------|---------------|
| 流程发现：系统实际在做什么？ | 根因推断：为什么出错？ |
| 一致性检查：是否违反 CONSTITUTION？ | 修复策略：该怎么修？ |
| 变体分布追踪：行为模式在变化吗？(→ P2) | 因果溯源：这个 bug 是上次改出来的吗？(→ P1) |

**关键原则：数据采集永远不用 LLM（避免幻觉），LLM 只做解读。**

---

## 6. 开放问题

**"房间里的大象"：人类用户输入**

当前框架假设纯闭环（系统观察自己 → 修改自己）。但 nanobot-auto 有 9 个通信渠道在接收人类指令。人类输入是最高优先级的外部信号。

这可能意味着 OCLSE 不是纯闭环，而是**带外部输入的半开放循环**，更接近**人机协同演化**。

---

## 7. 当前进度与路线图

```
已完成                          进行中              未开始
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  ━━━━━━━━━━━━━━━━━  ━━━━━━━━━━━━━
✅ 理论框架 (OCLSE + P1/P2/P3)  🔄 R&A 策略 (80%)   ○ 代码实现
✅ 架构分析 (Sense/Effect/Gate)                     ○ 实验运行 (14周)
✅ 度量体系 (H(t))                                  ○ 论文撰写
✅ SICA 论文评估
✅ Roadmap (7个里程碑)
```

下一步：解决"人类输入"问题 → 更新 Roadmap → 开始 M0（事件埋点）实现。
